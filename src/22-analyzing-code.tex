A topic worth discussing is how teachers or programs can analyze code. Analyzing in this context can be a somewhat vague term as it can easily confused with assessment. In assessment student performance is evaluated by specific criterion and students provided feedback in either informal or formal manner. Most common categories of assessment are formative and summative assessment. In formative assessment teachers assess and provide feedback during the student's learning process in for example lab sessions as guidance how to approach to solve a problem. Summative assessment is conducted at the end of learning activity in attempt to summarize student's learning regarding a study objective, mostly in the form of grading. In our case, the provided feedback to the submissions could be considered as formative. The submissions themselves are the final work for a given exercise but the feedback is non-graded and aimed to improve the students' programming skills over the duration of the course \cite{assessment}.

Discussing \textit{analyzing} the student submissions however, we refer to the general approach of reviewing the submissions without having to grade them or to provide feedback. We are simply interested in finding patterns that could be generalized as insights. \textit{Reviewing}, in our context, can be considered as a form of analysis conducted to assess student work which can include feedback or grading but which in our case is done mainly to help teachers to understand the student programming patterns better. In research methodology the analysis methods are often categorized as either qualitative or quantitative. Qualitative research is the analysis of non-numerical data by finding general properties and patterns that can be described as written conclusions. Quantitative research on the other hand involves numerical data or data which has been quantified as such which is analyzed by statistical methods to draw conclusions about the researched subject~\cite{strengths-limits-qual-quant}.

Program code is non-numerical data that has been researched in various ways by either qualitative or quantitative methods. Analysis has been applied in educational context to researching facets such as students' debugging skills~\cite{static-analyses-in-py-courses}, learning~\cite{static-analyses-in-py-courses}, misconceptions~\cite{programming-errors}, predicted grades~\cite{analyzing-student-patterns} as well as code quality~\cite{static-analyses-in-py-courses, alamutka-2005-auto-ass-survey} and complexity~\cite{alamutka-2005-auto-ass-survey}. Most of them have been qualitative in nature but for at least complexity quantitative methods have also been applied.

\subsection{Manual review}
\label{ssec:manual-review}

As stated in the introduction chapter, manual reviewing of programming submissions is often unfeasible given their large volume. However, if manual reviewing is necessary to for example grade the submissions, the process is often standardized to maintain consistent evaluation criteria. Grading rubrics are guides that are used to list the criteria how to evaluate the submissions, the scale on which different aspects are scored, the common mistakes and their effect on grades as well as feedback phrases. Reviewing is often conducted by multiple people, and grading rubrics help to reduce the effect of subjective biases of the reviewers \cite{zimmaro-rubrics-2004, aalto-rubyric-2009}.

For us, manual reviewing can be a source of valuable insight to optimize the automatization of the process. A good and widely accepted grading rubric could help to develop the similarity measures to cluster the submissions or to detect outliers. Also, the grades and feedbacks of the manually reviewed submissions could be used for creating models to automatically grade and provide feedback to the submissions.

Creating models to automatically grade and provide feedback is, however, not trivial. In a study by Rogers et al.~\cite{rogers-auto-style-2014} four semesters worth of submissions of an introductory Python course are analyzed, totaling at 1500 submissions. All the submissions have been previously manually reviewed by teaching assistants (TAs), and given a score between 0 to 3 and varying amount of feedback. They list 26 features or criteria that they have extracted from the feedbacks which they consider important, and develop a supervised learning model to grade and provide feedback to submissions automatically. The accuracy of their results is very low, more akin to random classification, and they note that many of the features are not classified consistently \cite{rogers-auto-style-2014}.

As one of the causes for the poor accuracy they identify the inconsistency of the grades and feedbacks. The TA who graded and provided feedback to the submission was by far the biggest factor, making generalizations difficult. They argue that automating feedback with supervised learning is very difficult and with lack of precision, the tool they created would serve best for analyzing the submissions rather than automatically scoring them \cite{rogers-auto-style-2014}.

In a study by Glassman et al.~\cite{glass-feature-engineering} TAs were given 50 submissions and asked to cluster them manually in any manner they wanted to. They note that the TAs clustered the submissions by structural similarities, overlooking the presentational details of the code such as which library functions were used. Subsequently, the clusters between two TAs were found not to be very similar and an automatic clustering approach only started to match the manual clusters with high enough $k$, such as 15, using k-means clustering. While this would indicate that there are distinct structural patterns how submissions could be clustered, it also shows that generalizing a model that would work well for a wide range of teachers is hard \cite{glass-feature-engineering}.

In another study Luxton-Reilly et al.~\cite{luxton-sub-variation-2013} find by analyzing the submissions of 10 simple program code exercises of an introductory CS course, that the variation between the submissions could be categorized into three distinct and hierarchical types. Structural variation is the highest level with the most variation, which is the control-flow structure of the program, represented by a control-flow graph. Syntax-level is the second highest, which is the variation of used program syntax that does not contribute to control-flow and it is only considered if the two submissions share the same control-flow structure. At the lowest level, if the syntax of the submissions would also match, the presentational-level would be the variation in aspects such as difference in formatting, the exact used tokens and their order \cite{luxton-sub-variation-2013}.

This study could indicate that in order to cluster the submissions by their similarities, the methods should have to be specified to certain variation level Also, the feedback could be defined as either belonging to a certain variation level or as a synthesis of multiple levels. Using a similar ontology might help writing better feedback and creating more accurate similarity detection models.

To summarize, manually reviewing the submissions is a good first step when building automatic grading or feedback system. The grades and feedbacks might be correlated to some structural patterns in the code yet their use for modeling can be problematic. The subjectivity of the reviews and their possible low quantity would not perhaps lend itself for good classification models. Analyzing and understanding the qualitative criteria how people intuitively cluster the submissions by hand would still seem very valuable. 

\subsection{Programmatic review}

Automatic analysis tools are important applications for both code and natural language text to evaluate their quality. In the context of program language code these are commonly categorized as static or dynamic analyzers, where static analysis is the analysis of the programs without executing them and dynamic analysis during their execution \cite{static-dynamic-hybrid-analysis}. Dynamic analysis is mostly concerned with performance although OverCode uses dynamic analysis of the execution graphs to canonicalize variable names\cite{overcode}. The automated tests could be considered as a form of dynamic analysis. Due to the difficulty of applying dynamic analysis as it is heavily dependent on the programming language and its execution model, we shall focus on only the static analysis and its applications.

One popular type of static analysis tool is a program called "linter", the name derived from a Unix C style checker named \texttt{lint}~\cite{lint-1988}. It provides small feedback messages how to properly format or fix segments of the code that it could even do automatically. This helps developers to maintain uniform style, reduce maintenance, enhance readability and to avoid errors in their code \cite{lint-1988}. More advanced static analyzers can find very concealed and complex problems with the codebase based on extensive data and heuristics on what causes bugs~\cite{fb-static-dynamic-analysis}.

Could the teachers' reviewing of the submissions be defined as static analysis of poor formatting, inefficient computation or bugs in the code? Unlike with the industrial grade static analyzers, the motivation for teachers is pedagogical and not the heavy constraints of having to ship robust and efficient code in a standard manner. It could even be seen as counterproductive to require the same standard of quality from the students as from highly skilled software engineers. And even if the students were able to fix their code with the help of a very good static analyzer, how much would they benefit from the mechanical task of blindly following the tips of the analyzer?

Certainly, some standard of quality has to be required from the students yet the justifications for it are necessary. Adding a strict style checker as part of the automated test suites would probably not be immensely helpful for helping the students to learn, who would be overburdened by having to solve the exercises and fix seemingly arbitrary style errors as well. Creating a static analyzer with pedagogically good error messages could be challenging, but they have been implemented in some cases \cite{static-analyses-in-py-courses, fox-roy-autostyle-msc-2016}. We outline the benefits of static analyzers as a possible future research topic in the Subsection~7.1.2.

A major obstacle in automating this form of qualitative analysis would be its subjectivity. For example, generalizing an aspect such as complexity as overly long and verbose code is problematic as it could happen that longer code was more readable than shorter. Therefore, it seems implausible that teachers could be able to create a set of feedback messages and patterns that would be comprehensive and specific enough and also apply to all the exercises in a given course. Specifying the analysis on exercise basis would probably provide teachers more flexibility and freedom in generating the messages and their patterns.

In the context of a single exercise, this method might work relatively well. However, this would require considerable time and effort from the teachers to find patterns from the code that were consistent enough so that they could be used as indicators of for example misconceptions or problems in the code. Creating and maintaining a very large collection of these patterns and their messages would seem very similar to creating an additional set of automated tests, which benefits and drawbacks were discussed in the introduction chapter. Another question is, would the students appreciate such automatization? In theory, better feedback should be beneficial to students' learning and motivation but if the students would see the messages only after they had received their points, they could see it as extra-work. Therefore, the exact approach to implement static analysis would have to be carefully planned.

Crow et al. note that in many introduction programming courses code quality is not even necessarily taught to the students \cite{crow-code-quality-2020}. Therefore, the students might not even know how to write readable code that is clean, modular and uncomplex. A static analyzer could indeed help to inform students about the quality of their code, but it also should be the responsibility of the teachers and the course material.

Instead of a static analyzer, an intelligent tutoring system (ITS) could provide more general real-time formative feedback to the students while they were solving the exercises. Rather than only parsing patterns and providing canned messages, it could be incorporated as a more holistic help that used the metadata of failed submissions and historical student performance. Thus it could provide tailored help whenever student failed to progress in the exercise or just general advice, similar to how students would receive help in lab sessions. Creating an ITS would require extensive data of incomplete or buggy submissions at points where the students have struggled the most. Also it would require substantial effort and expertise to build, and it might not scale well for exercises with large solution spaces \cite{glassman-reusable-feedback, its-2020}.

Glassman et al. for example propose MistakeBrowser and FixPropagator that enable the analysis of failed submissions to find the shortest number of transformations that would fix them, and allow teachers to provide feedback to students for that specific problem \cite{glassman-reusable-feedback}. Ngueyn et al. also propose a system for real-time feedback that is built on top of Codewebs \cite{nguyen-real-time-feedback}. The benefit of ITS compared to static analyzer would be the depth of the possible feedback provided to the student, yet they are not mutually exclusive. What, however, would be important, is the real-timeness of the feedback. If the feedback was delayed until the student had submitted their code, or by days or weeks, the student might not internalize it very deeply. Real-timeness might also be more effective in preventing the students from forming bad habits.

\iffalse
xcite luxton Understanding Semantic Style by Analysing Student Code Who also found 16 different semantical indicators of potential lack of knowledge.

Data-driven Feedback
Treating feedback generation as a classification task allows us to address RQ1 and evaluate the extent to which SourceCheck agrees with (predicts) the feedback of human tutors. The results of this evaluation would be difficult to interpret without a baseline for comparison

RQ1: How well does SourceCheck’s feedback agree with ideal human tutor feedback? SourceCheck agrees with approximately half of ideal tutor feedback provided in Phase I, almost as much as another human tutor, with SourceCheck achieving a recall 76 and 88\% as high as TC on GG and SQ respectively. This does not necessarily mean that SourceCheck’s feedback is almost as good as a tutor’s. It is possible that when SourceCheck’s feedback diverges from a tutor’s, it does so in a less useful way than when another tutor does so; however, this is difficult to investigate without some direct measure of hint quality (e.g. [8]). For now, we can say that these results suggest good potential for data-driven feedback generation, in that ideal tutor feedback is frequentlycontained in the set of edits generated by SourceCheck.
\fi
