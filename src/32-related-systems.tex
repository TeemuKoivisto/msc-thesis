Various systems have been created over the years that provide analysis and clustered feedback for programming of natural language submissions. Still, no single system has become a \textit{de facto} standard for this purpose, and most systems have not seen use outside their research study setting. The following three systems presented here were chosen based on their clear architectural descriptions as well as the completeness of their implementations.

Of the three systems, OverCode has the most comprehensive documentation with all its source code public, the license however being unspecified. Codewebs's code is also public and open-sourced under MIT license but lacking documentation. Divide and Correct has perhaps the clearest architectural description but there was no open-source code found to exist. Divide and Correct is also based on natural language submissions instead of program code, but because their method for similarity detection is well-documented and applicable to code, we consider it as a valid system to review \cite{overcode, codewebs, divide-and-correct}. Next this section goes through these three systems in more detail to consider their applicability as well as possible drawbacks.

\subsection{OverCode}

OverCode~(2014)~\cite{overcode} is a code visualization and exploration tool for MOOCs, designed to help teachers and TAs to create grading rubrics or to find misconceptions, pedagogical examples and other interesting patterns. Implemented in and used for Python code, OverCode uses various static and dynamic analysis techniques to reduce the dimensionality of the submissions. As submissions are ingested by the system, OverCode executes them and by analyzing their execution graphs it discerns variables that are used similarly to rename them to one common name. It also allows creating custom rewrite-rules to merge clusters together.

The main units that teachers use in OverCode are the \textbf{stacks}, which are single lines of canonicalized code that are shown with the number of submissions they occur in. Teachers can select stacks to write feedback to the submissions, merge them with other stacks or just set them processed. A progress counter keeps track of the percentage how many submissions have been processed. Unlike other systems that use more abstract methods such as TF-IDF matrices, OverCode keeps the code very close to the original representation by pattern matching the canonicalized lines of code thus making the process very tangible to teachers.

The downside, however, of using very simple blocks to cluster the code is that OverCode might not generalize well outside the introductory MIT Python programming course it was tested on. As the exercises would become more complicated in the advanced courses, the variation between the solutions would also increase. Therefore, they might not share very similar variables that could be renamed, or lines of code that are very similar. Also, OverCode does not discern the overall structure of the code which makes it hard for the teachers to view the higher-level student approaches.

We did not attempt to replicate OverCode's functionality in CodeClusters with canonicalizing the variable names or separating them into stacks. The generalizability of the method was not deemed optimal for supporting various courses with multiple programming languages, which each would have required specific implementation for the execution graph analysis. The method also seemed hard to change or customize after its implementation. The UI choices OverCode effected the development of CodeClusters to some extent, and its simplicity was seen as an important aspect to emulate. Its available public source code was inspected yet due to its age and architecture it was not trivial to decipher, and we failed to run the application locally.

\subsection{Codewebs}
\label{ssec:codewebs}

Codewebs~(2014)~\cite{codewebs} is a homework search engine for indexing programming submissions and searching them using probabilistic information retrieval. The premise on which Codewebs is built upon is the observation that within a single, well-defined exercise the submissions contain a lot overlapping structure. By identifying the shared parts a much sparser set of decision points can be found, which defines the higher-level structure of the submissions. The teachers can then use Codewebs to find these structures to provide feedback to students on for example misconceptions or other problems.

For finding these decision points Codewebs employs various methods. At its core, Codewebs is a search engine that uses inverted index to store the submissions. All the submissions are run with a series of unit tests and their results along with the code are added to the index, both as strings and as parsed ASTs. The ASTs are not represented as graphs in the index but as linear arrays, which makes their retrieval a lot more efficient.

To retrieve documents, the user selects a seed set of \textit{code phrases} they think are semantically similar as their search query. These code phrases are subgraphs of ASTs than can be either subtrees, subforests or contexts of an AST. The user can for example select an expression \texttt{(X * theta - y)} as their query. Then, Codewebs finds the most relevant documents to that seed set by finding the exact matches of that AST as well as those that are approximately similar by a method they refer to as \textbf{semantic equivalence}.

For two submissions to be considered as semantically equivalent they need to share similar results on a portion of the unit tests. Also, the ASTs they appear in have to have a high enough probability of being submitted to the same problem. Codewebs then finds the matching ASTs subgraphs as well as the contexts they appear in, iterating over them until all the possible subgraphs and their contexts have been found that might be similar to a given seed set. For example, ASTs with a phrase \texttt{(theta' * X' - y')'} could appear in the results and teacher could provide force-multiplied feedback to all the submissions at once.

The approach Codewebs presents is quite unique with a lot of potential for further improvements. However, while they tested their system on a dataset of million submissions of Octave code, it is still questionable how well the system generalizes to other programming languages and courses. One of the main requirements they state is a large number of submissions which might not be achievable with an average MOOC course. Another constraint of Codewebs is that it is designed to work with submissions that contain a single function which is highly similar across the submissions. If, however, the exercise would be longer and the students divided their code into multiple functions, the probabilistic matching might fail to find similar AST contexts. Thirdly, the use of unit test results as part of the retrieval makes the system hard to extend to impartial exercises which do not pass the tests, and they have outlined this as possible area of future work.

To summarize, Codewebs is a highly scalable search engine to grade, analyze and provide feedback to submissions. Yet, since pursuing a similar direction would have tied CodeClusters to a specific search and similarity detection mechanism, we did not attempt to replicate their approach. Building a complete search engine in itself would have required considerable effort which would have been further inhibited by Codewebs's lack of documentation. We did not also have test results as part of our dataset, making the use of the same similarity detection method unfeasible.

\subsection{Divide and Correct}

Divide and Correct by Brooks~(2014)~\cite{divide-and-correct} is a system to read, grade and provide feedback to short answer natural language submissions at scale. It applies similarity detection by combining multiple features and transforming them into a single distance metric, after which they are clustered using k-medoids algorithm. Using a web UI the users can grade and provide feedback to the individual clusters.

The unique features of Divide and Correct include its hierarchical clustering which allows teachers to first divide the submissions into up to 10 clusters and then further divide them into maximum of 5 subclusters per cluster. By constraining the number of clusters and subclusters the complexity of the task should stay relatively stable regardless of the number of submissions. If a submission does not fit into any of the clusters, it is put into a miscellaneous cluster or subcluster.

Divide and Correct uses for its similarity detection a distance metric computed for all the pairs of submissions using length, shared words, TF-IDF term similarity, string matching and Wikipedia based LSA similarity. The clusters themselves are created with k-medoids clustering algorithm with some minor modifications. This approach is based on their previous research on a novel clustering method Powergrading \cite{basu-powergrading-2013}.

To grade and to provide feedback to the submissions teachers would choose a higher-level cluster from the UI first, whose subclusters would then automatically inherit the same grade and feedback. Teachers could then go through the subclusters and change the grade and feedback on a subcluster basis. Although not recommended, teachers could also go through the individual submissions and change their grade and feedback. This, however, would eliminate a lot of the benefits of the clustering.

The system was evaluated by comparing it to a flat grading system in which the submissions were processed one by one, their test users consisting of 25 teachers with various backgrounds and the test dataset of 2 exercises which each contained 200 submissions. The study was conducted as a within-subjects study in which the teachers used both systems for grading and providing feedback to the submissions. The first group of 10 teachers started with Divide and Correct, the other 15 with the flat grading system. A golden standard from Powergrading Corpus was used as a baseline for grades.

They found the grades to be very similar between the groups and consistent with the baseline. Using Divide and Correct, however, teachers provided a lot more feedback compared to the flat grading and the teachers also noted it made providing feedback easier. A major difference between the groups was the time it took for teachers to perform the task, using Divide and Correct teachers were able to go through the submissions roughly 3 times faster. In the post-study questionnaire majority of the teachers preferred Divide and Correct to the flat grading, 21 out of 25 considering it better. One of the complaints teachers had for Divide and Correct was the complexity of working with the clusters, as it sometimes led to a lot of rechecking and backtracking on the answers.

Divide and Correct appears as a well-rounded approach to grade, feedback and cluster natural language submissions with various similarities to CodeClusters. They both use term-based TF-IDF matrices for similarity detection, allow clustering with k-means type of algorithm and the submissions from one cluster are selectable from the UI to write feedback from one view.

Since Divide and Correct was designed for natural language text submissions instead of code, some parts of its approach are less feasible to replicate. For example, the use of the same similarity detection algorithm is unfeasible as it is tailored for natural language. Also, CodeClusters is designed to allow experimentation with multiple clustering algorithms and it could not be specialized for one type of algorithm. Using hierarchical levels for clusters and constraining their number seemed as a very interesting idea to experiment but it would have required considerable engineering effort to implement it without making the system less flexible.

While promising, Divide and Correct suffers from the same problem as OverCode in that while it produced good results with a very specific dataset, the overall solution might not be generalizable for a wide range of heterogenous datasets. The combining of multiple features into a single distance metric seems as a promising possible future enhancement to CodeClusters, which might improve its similarity detection models. Sadly, with lack of open-source code and visualizations it was hard to assess the full validity of their system and how easily their approach could have been customized for code instead of natural language text.
