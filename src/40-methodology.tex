This chapter describes the design, methodology and the structure of the study. As part of this thesis we have contributed a new system, CodeClusters, which we evaluate as well as the problem we have proposed: teachers are unable to review the large quantities of submissions in programming courses. While this thesis could be described as design science research of how to design a system to analyze, cluster and provide feedback to program code submissions, this is not sufficient in itself to describe our research methods.

\textbf{Design science research (DSR)} is an information systems (IS) research methodology used primarily for designing software but also various processes and methods. In for example natural sciences the goal is to uncover universal truths about nature and environment, and in social sciences, such as psychology, from societies and individuals. In DSR the aim is to research man-made artifacts and to evaluate their novel innovations. While DSR is quite loosely defined, as the definition below shows, it provides a theoretical framework to serve as a basis for our methodology~\cite{drs-in-is, drs-drm}.

\begin{displayquote}
\label{qt:dsr}
"\textit{Design science research is a research paradigm in which a designer answers questions relevant to human problems via the creation of innovative artifacts, thereby contributing new knowledge to the body of scientific evidence. The designed artifacts are both useful and fundamental in understanding that problem}~\cite{drs-in-is}."
\end{displayquote}

\iffalse
More sensible definition would be, that the goal in DSR is either to design a new artifact or an improvement to existing artifact to provide a (impartial) solution to a problem that is better than existing artifacts in some aspects. This in contrast to artifacts whose aims is to imitate existing designs without proposing improving the current designs. A more concrete analogue could be, that DSR is the designing process of product development while routine designs are reproductions of existing designs, more akin to manufacturing. Of course, designing in abstract is just the process of proposing 
\fi

The difficulty of applying DSR are its vague formalisms that are closer to best practices than exact criteria which may differ from researcher to researcher. It has been stated~\cite{drs-in-is} that to be considered as DSR, the artifacts must propose novel innovations compared to routine designs, but from user's perspective the novelty of the artifact is auxiliary to its impact and usefulness. Unless novelty is considered to have intrinsic value on its own, one can even consider novelty redundant when measuring the benefits achieved from designing and using a tool for a task. Perhaps the difference is that in DSR the focus is to create designs that offer improvements to the existing designs whereas with routine designs the focus is to emulate or reproduce existing designs. However, this inexactness makes following DSR formally quite problematic and, therefore, our research simplifies it to large extent. Our approach can be defined as exploratory DSR analysis which main goals are to verify that the problem we have proposed exists and that the designed artifact has potential to solve that problem to varying extent\cite{drs-drm, drs-in-is}.

The stakeholders of the artifact are mainly the teachers, but also the software developers who would have to develop and maintain the artifact as well as students who would receive feedback. There are no exact requirements that the artifact must meet, although we expect the artifact as a whole to be perceived as useful from the teachers' point of view. The contributions of the artifact include its open-source code, extensive documentation, working prototype as well as various features to explore, model and provide feedback to submissions.

It has been proposed that the DSR design cycle of artifacts consists of 3 main activities. In the relevancy cycle the problems are identified and validated as well as the desired outcomes proposed for the artifact. In the design cycle a background research is conducted into the current approaches and best practices and possible solutions are outlined. In the rigor cycle a new design or an improvement on existing design is designed - an artifact - during design iterations in which it is developed until a satisfactory design is produced. After the artifact has been developed and thoroughly tested, it can be released for field testing to evaluate it in a new relevancy cycle to start the design cycle from the beginning \cite{drs-in-is}.

In the introduction chapter we identified a research problem that was based on a proposal of the thesis supervisor which was also found in the published research. We divided it into 4 research questions that each included a different aspect of the problem with the aim of researching and designing a prototype system to demonstrate a possible solution. Next, we conducted a literature review which produced a comprehensive overview of the research and systems related to the topics of the research questions. Various possible approaches were found from which we picked the most versatile and realistic options. During this process the artifact, CodeClusters, was developed whose design was influenced by the research and vice versa. And lastly, we now propose the methods to evaluate the artifact.

We conduct a user study to evaluate the existence of the problem as well as the suitability of our artifact for solving this problem. This is done as interviews with the target users of our artifact, teachers of programming courses. The sample size for our study is small, 5 teachers, to guide the design process of the next iteration of the artifact rather than to evaluate its completeness.

In addition to this study, we also use a set of user interface design criteria proposed by Gram and Cockton~\cite{drs-drm, drs-gram-cockton-1996} to evaluate our artifact from user and software engineer perspective. While the criteria are not exhaustive of every possible aspect of an UI, they help to verify how well CodeClusters satisfies the basic requirements for a software product. Other more formal evaluation criteria, such as ISO/IEC 25010:2011\footnote{\url{https://www.iso.org/standard/35733.html}} software product evaluation or ISO 9241-210:2019\footnote{\url{https://www.iso.org/standard/77520.html}} human-centric design standards, are not used since they appear unnecessarily complicated and also incur expenses.

All the participants for the study are selected from the employees of Aalto University and University of Helsinki as they would be the most likely users of the system. The interviews were conducted in the August of 2020 using Google Hangouts or Zoom, Google Docs, Google Forms and a hosted CodeClusters application\footnote{\url{https://codeclusters.cs.aalto.fi}} with the following interview structure:

\begin{enumerate}
  \item Background questionnaire
  \item Performing of tasks using CodeClusters
  \item User satisfaction and system evaluation questionnaire
\end{enumerate}

Next this chapter goes through the different parts of the interviews in more detail, but to summarize the questionnaires are simple surveys that evaluate the context of our problem as well the perceptions of the teachers regarding the artifact. The tasks are scripted scenarios that the teachers are asked to perform to evaluate how easy and intuitive the UI is to use. The interviews are also recorded, transcribed and coded to find any thematically important ideas and observations teachers say during the interviews. The questionnaires, how users performed during the tasks and the coded interviews are then aggregated into one qualitative analysis, namely thematic synthesis, to present the overall picture of the opinions of teachers regarding the problem as well as the artifact. The interview questions and the tasks are provided in the appendix.

The analysis is described in the results Chapter \ref{ch:results} which, in combination with the literature review and reviewing of related systems, are used to answer the original research questions in the discussion Chapter \ref{ch:discussion}.

\section{Structure of the interview}

The interviews could be defined as semi-structured with the interview questions, however, being given in the form of online questionnaires instead of verbally. Teachers are free to discuss the questions at any given point and to give further justifications for their answers. Also, they are asked after both questionnaires if they have something to add regarding the themes and the questions. The questionnaires should take a fairly short amount of time which is desirable as the tasks would in turn be considerably long.

\subsection{Background questionnaire}

The purpose of the background questionnaire is to ensure that the participants have indeed experience in teaching programming courses and to analyze the perceptions teachers have regarding grading and providing feedback. The questionnaire consists of 14 questions: 5 likert-scale questions with 5-levels, 3 free-form text fields, 1 multi-choice question with free-form text "Other" option and 5 single-choice questions with 5 options, one with 4. The questions are described in the Appendix \ref{appendix:questions-1}.

It is designed to be a simple questionnaire to gauge the general impressions of the teachers without requiring them to analyze their opinions in too much detail. It would seem unreasonable to inquire the teachers in much further depth than that as the topic, from prior knowledge, is expected to be unfamiliar to the teachers and no similar systems are known to be in use.

\subsection{CodeClusters tasks}

An important part of the evaluation of software is ensuring that it fulfills the basic requirements of functioning and usability by various users. CodeClusters tasks are designed to verify these requirements by simulating an imaginary task of reviewing Java programming course exercise submissions by finding outliers as well as providing feedback using similarity detection and clustering. The 110 submissions used in the tasks are solving a variation of the Rainfall Problem, which is considered as a relatively difficult exercise for beginners \cite{rainfall-2015}. The use of very scripted tasks instead of a more free-form approach allows limiting the time required to conduct the interview and it should also make the teachers feel less overwhelmed using an unfamiliar system.

The tasks are divided into 4 main parts with each part addressing a different aspect of the system and they are in order: search, modeling, review flows and reviews. To perform them, the teachers login to a hosted CodeClusters service that is seeded with the test dataset and reset between the interviews. The tasks can be found in their entirety in the Appendix \ref{appendix:cc-tasks}.

The first part of the tasks consists of searching code using the Solr search server. They require teachers to find the correct controls and to use them to perform various searches. Teachers will use various features of the Lucene API, such as facets and filters, to showcase the search capabilities of CodeClusters. 

In the second part the n-grams model is executed to create clusters of submissions. Teachers are not expected to understand the model, or its implementation, and all the parameters are provided in the task description. In the tasks they are asked to inspect the generated clusters and to compare them against each other. The goal is to see how well the teachers can process the clusters and do they find any structural patterns amongst them. The tasks should demonstrate how such the models behave in general and what are their possible shortcomings.

In the third part teachers use the review flow functionality of CodeClusters. Review flows are a unique feature of CodeClusters that allow the composition of search, modeling and review into a single executable action that would help to further automate the repetitive parts of the review process. They are asked to create a very basic review flow to demonstrate its functionality.

And lastly, the teachers are asked to test the review overview functionality which allows them to see all the submissions in a single view with the provided feedback. They are asked to edit a few of the associations between the reviews and submissions and then to accept them.

\subsection{User satisfaction and system evaluation questionnaire}

The third part of the interview is the user satisfaction and system evaluation questionnaire, which consists of 9 questions. These are 5 likert-scale questions with 5 levels, 3 free-form text fields and 1 multi-choice question with free-form text "Other" option. The questions are described in the Appendix \ref{appendix:questions-2}. The teachers are asked for their impressions regarding CodeClusters and how they would see its features, the search, modeling and reviewing, be of use for analyzing and providing feedback to course submissions.

\section{Analysis of the results}

For the analysis we combine the results of the coded transcripts of the interviews, the questionnaire answers and the performance of the teachers during the tasks. We process the questionnaires and tasks chronologically and synthesize all the related information per major topic. Any statistical methods are not used due to the low sample size and the inexactness of the measured phenomenon. We also evaluate the artifact by UI usability criteria to affirm it fulfills the basic requirements for a software product. Without statistical methods the results are hard to verify for veracity, yet they should provide a general impression to validate the problem as well as to guide the next iteration of the artifact.
