We already discussed how to represent unstructured data such as code in an IR system in the Section \ref{sec:ir}. In vector space model, each document was transformed into an array of tokens that were represented as vectors of frequencies. Yet because program code is in fact instructions for the compiler how to run the program, it has hierarchical structure that is very precisely defined by its grammar. When the compiler parses the code, it transforms it into one or multiple intermediate representations, such as abstract syntax tree (AST), that help in type checking, optimizing and analyzing the code. Higher-level intermediate representations are easier to optimize and manage than lower-level representations, which is why syntax trees are often used instead of linear representations or machine code \cite{aho2007compilers}.

Another form to represent code is the control-flow graph that represents the execution path of the program. They are used by the compiler to analyze and optimize the program, as well as how to schedule and allocate memory \cite{engineering-compiler}.

And as described in the IR Section, the code could also be treated as an array of string tokens. It can be generated by parsing the AST and then processing it in depth-first fashion by pushing its nodes into an array. While it loses its original tree-structure, arrays provide much greater flexibility in using various machine learning models. Using n-grams instead of single tokens allow retaining some of the original structure.

This section goes through these three selected representations in more detail to understand their differences, and what benefits and drawbacks each have for representing code. Most of this section is based on the books \textit{Engineering a Compiler: 2nd Edition} (2013) by Cooper et al.~\cite{engineering-compiler} and \textit{Compilers: Principles, Techniques, and Tools: 2nd Edition} (2006) by Aho et al.~\cite{aho2007compilers}.

\subsection{Abstract syntax tree}
\label{ssec:ast}

Executing program code is a multi-staged process that is done differently in many programming languages and environments. Some languages, like Java, use just-in-time (JIT) compilation and run the program on a virtual machine. Others, such as C, are compiled to a single executable binary that is customized for a specific operating system and processor architecture. But regardless of the details of the process, in-between the parsing and executing the program, one or more intermediary representations (IR) for code are used \cite{aho2007compilers}.

In general, the program code is first parsed into a stream of tokens, the basic syntactical units of the language, by a lexer. Then, the tokens are parsed into a parse tree by a parser following the grammar of the programming language. Because parse trees contain the whole derivation of the grammar, they can become very large and bulky which is which they are often transformed into more compact abstract syntax trees (ASTs) \cite{aho2007compilers, engineering-compiler}.

ASTs eliminate much of the redundancy of the parse trees and offer a good abstraction level for the compiler to analyze and optimize the program. Some compiler architectures use ASTs for executing the program while others transform it to another, lower-level IR, before execution. The AST could be turned into a directed acyclic graph (DAG) which is sometimes done by the compiler, but for us a normal undirected graph would be the most flexible to use. As AST is a tree all its directed edges are from the parent nodes to their children which makes that information redundant. Therefore, using an undirected graph would be preferable as it is more efficient to use and available to more graph algorithms \cite{aho2007compilers, engineering-compiler}.

While a graph of AST might be easier to use than tree, from modeling perspective the size of the graph would still be too large. Many of the graph algorithms, such as tree-edit distance, are not exactly fast with worst case complexity of $O(n^4)$ \cite{ted-tutorial-2018} which might lead to very slow run times. A lot of the tokens might also not contribute any meaningful signal in the detection of outliers or similarities, and thus they could be regarded as noise. Semantically similar tokens are also prevalent in ASTs, which makes discerning similarities hard. 

As a solution to this problem systems such as MistakeBrowser~\cite{glassman-reusable-feedback} and Codewebs~\cite{codewebs} standardize the ASTs by removing the less important tokens, which could be seen corollary to removing stop-words in natural language processing (NLP). Then, similar tokens are transformed into a single representation, such as all \texttt{for} and \texttt{while} tokens becoming \texttt{loop} tokens.

JPlag, a Java plagiarism detection library, has a list of 39 tokens that it only considers in their detection algorithm and a larger set of 82 tokens that can be optionally selected by the user. They note that while the larger set does capture more structural features, it also increases noise which might lead to misclassifications \cite{jplag}. CodeClusters allows the use of three sets of tokens: language keywords, all available AST tokens and a version of JPlag \cite{wahlroos-2019}. The JPlag version seemed to produce the most reliable results.

To summarize, ASTs offer the full syntactical structure of the program in a hierarchical form. Because they can be rather large and noisy from the modeling perspective, they are often pruned and standardized before used in models. This helps to combat the curse of dimensionality and to increase accuracy in detecting semantically similar code. Since the results between the different sets of used tokens might vary, they have to be carefully crafted and analyzed by for example offering multiple different sets for the user to choose. 

\subsection{Control-flow graph}

Knowing the execution paths of a program is very helpful when analyzing it for optimization. Normally, this is modeled as a control-flow graph (CFG), which is a directed cyclic graph of all the possible execution paths a program may take during its execution. Its nodes are the basic blocks of a program that are sections of code always run in sequence with control entering at its first operation and leaving at the last. Every edge in the graph is a possible transfer of control between the blocks \cite{control-flow-analysis, aho2007compilers, engineering-compiler}.

CFGs have traditionally been used for compiler optimization and static analysis, as they portray the actual execution of the program, unperturbed by the syntactical noise \cite{control-flow-analysis, engineering-compiler}. Cyclomatic complexity uses CFGs in its calculation of complexity \cite{mccabe-1976}.

In our case, CFGs might be better suited for structural analysis of the submissions compared to ASTs. They remove many of the noisy details of formatting and syntax leaving out the actual behavior of the program. CFGs have also been used to label variations between student submissions \cite{glass-feature-engineering, luxton-sub-variation-2013}.

Losing the tokens, however, also implies losing the ability to detect any syntactical similarities. This would decrease the overall applicability of models that use only CFGs. Attempts have been made to join the two into a graph that shows the control flow structure of the program while retaining many of the tokens of the AST. Control-Flow Abstract Syntax Tree (CFAST) \cite{cfast} for example models the control structure of the code by pruning ASTs to only contain the tokens that contribute to the control flow of the program.

\subsection{Token array}
\label{ssec:array}

As described in the vector space model Subsection \ref{ssec:vsm}, code can be transformed into a sequence of tokens $d=(t_1, ..., t_n), t\in L$ where $L$ is the set of available tokens. Being an ordered list, it could be stored as an array and trivially transformed into a term-frequency matrix. From the compiler perspective, array is not very sufficient for representing code as it loses the hierarchical structure of the program. Only at the beginning of the compilation step the tokens are parsed to an array by the lexer. Therefore, compilers do not output it as such, and it has to be generated outside of the compiler by processing the AST using depth-first search and pushing its nodes into an array \cite{engineering-compiler, aho2007compilers}.

While lacking the structure of the graph, the array offers many exciting opportunities from modeling perspective as it is very similar to text, hence allowing the use of many text mining or NLP algorithms. Its linearity also makes it fast to process compared to graphs, which are not as easy to process in parallel\cite{parallel-graphs-2018}.

However, because the original ASTs were hierarchical trees, one-dimensional arrays lose a lot of structural information about the program. This might lead to poor performance in detecting structural similarities. Especially with representations such as the TF-IDF matrix, where the vectors are treated unordered thus losing not only the hierarchical structure but also the sequential order.

Some systems, such as JPlag, try to alleviate this problem by using separate tokens for the start and end of statements, such as "BEGIN\_IF" and "END\_IF" \cite{jplag}. This helps detecting nestedness in the data and in combination with n-grams, which is described in the Subsection \ref{ssec:ngrams}, the tokens are able to retain some of the original structure and order.
