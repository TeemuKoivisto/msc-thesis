This chapter discusses the results of the interviews, as well the findings of the background research and the design of CodeClusters. We synthesize them in the context of the original research questions and evaluate how sufficiently we are able to answer them.

\section{Answering the research questions}

As stated in the introduction chapter, the subject of this thesis has been the many facets of reviewing and providing feedback to large quantities of submissions in programming courses. This topic was divided into four research questions that each address a different part of the problem. Through literature review in Chapter \ref{ch:background}, building a new system as described in Chapter \ref{ch:system-description}, reviewing of related systems in Section \ref{sec:related-systems} and our study, we have gathered a solid understanding about of the context of our problem as well the practicalities involved in solving it.

\bigskip
\noindent
\textbf{RQ1: How code can be represented for efficient search and modeling?}
\bigskip

\noindent
There are a multitude of options to represent code which each suit different purposes. We discussed the basics of information retrieval and search in Section \ref{sec:ir} in which we used arrays to represent text or code. Since search involves mostly text or other unstructured data, an array is an efficient solution to process and query large quantities of documents. Parsing the text into tokens and transforming them into vectors of a vector space model allowed the use of TF-IDF matrices which are flexible yet not always accurate representations of documents.

But in contrast to natural language text, our documents are program code which follow a very strict grammar and hierarchy that allow compilers to parse and execute them. AST is perhaps the most well-known intermediary representation which represents the full hierarchical form of the program which can also be represented as a graph. Another common form is the CFG which represents the execution paths of the program. For search, graphs may not be very sufficient to use as such but as demonstrated by Codewebs, the hierarchical form of the graphs can be leveraged to find subtrees or contexts of ASTs\cite{codewebs}.

For modeling, arrays as well as graphs are both suitable although graphs heavily restrict the available algorithms. Quantifying the code into numeric values, metrics, makes it easy to process, although their applicability for discerning structure or other deep phenomenon is lacking. In outlier detection they might be useful as any values over or below average might reveal unique solutions to the exercise, which using CodeClusters and cyclomatic complexity seemed to find unusual submissions. The main focus of the models in this thesis has been similarity detection to find structural similarities to provide clustered feedback to multiple submissions at once. For that purpose, graphs or arrays were the most common representations found in the related research\cite{fox-clust-leverage-2015, fox-roy-autostyle-msc-2016, codewebs, divide-and-correct, glassman-reusable-feedback}.

TED\cite{zhang-et-al-1989} is a popular graph algorithm for code similarity detection which generates a matrix of edit-distances for a set of graphs. It is, however, very expensive to run with a worst-case complexity of approximately $O(n^3)$ \cite{ted-demaine}. For similarity detection of submissions, Fox et al. noted that a weighted version of TED produced more reliable results than the basic version, although there was no mention of the execution time for their 800 Python submissions \cite{fox-clust-leverage-2015}. In small scale, weighted TED might be suitable although the experience with CodeClusters showed that the implementation should be carefully crafted to achieve fast execution times utilizing perhaps multi-threaded processing.

As an alternative, arrays are a lot more efficient to process and they can be easily scaled to large datasets as demonstrated by Codewebs with a dataset of million submissions \cite{codewebs}. To score similarities between arrays, GST\cite{wise-gst-1993} could be used yet as the experiments with TED showed, its high worst-case complexity and poor ability to find higher-level structures might make it non-optimal. The major problem of arrays is that they lose the original structure of the AST or CFG.

Generating n-grams of the token arrays would retain some of the structure of the original graphs. These could be generated sequentially, or as proposed by Karnalim and Simon~\cite{simon-better-ngrams-2020}, hierarchically from the AST. This could improve the detection of higher-level structures in the code. Similarly using CFGs or a combination of CFGs and ASTs might improve the detection of higher-level patterns \cite{cfast}. Tuning the used set of tokens might produce better results which were experimented with CodeClusters\cite{jplag, wahlroos-2019}. The used model in CodeClusters also used ranges of n-grams that seemed to generate more balanced results compared to a single value of $n$.

These n-grams could be transformed into a vector space model, similar to search, and use cosine similarity and dimensionality reduction to improve the results. As TF-IDF matrices, however, the arrays would also lose their sequential order in addition to losing the hierarchical structure of the graphs. Generating n-grams instead of single tokens might mitigate this to some extent, yet arrays cannot fully replace the original graphs. Combining the n-grams with other features could be incorporated in the model, as demonstrated by Divide and Correct~\cite{divide-and-correct}. There might exist NLP methods for text that could be applicable to code as well.

\bigskip
\noindent
\textbf{RQ2: What methods can be used to analyze and cluster code?}
\bigskip

\noindent
The literature review provides a multitude of possible approaches to analyze as well as to cluster code. For the analysis, we categorized the approaches in the Section \ref{sec:analyzing-code} as either manual or programmatic review. The manual reviewing might be very monotonic and time-consuming and it is what thesis aims to avoid by designing a novel approach and system. Yet, it could be worthwhile to manually review some of the student submissions to understand how to automate the process \cite{glass-feature-engineering, rogers-auto-style-2014, luxton-sub-variation-2013}.

For programmatic review, we mostly discussed static analysis to analyze the submissions with pattern-based matching to reveal problems in the code and to provide feedback messages. A popular static analyzer is the linter\cite{lint-1988}, which evaluates stylistic and quality aspects of the code. Static analysis might provide very extensive information about latent errors in the programs which are not immediately obvious to developers, and they are very popular in the software development industry \cite{fb-static-dynamic-analysis}.

Using a similar static analyzer in programming courses might not, however, be as desirable as students could find it hard to follow the advice of the analyzers as well as to complete the exercise. Requiring the same level of rigour from students as from software engineers might be unwarranted. Also, the students might have not been even instructed on quality concepts during the course\cite{crow-code-quality-2020}. Static analyzers have been customized for pedagogical purposes in some cases\cite{static-analyses-in-py-courses}. The effort required to customize a static analyzer might be substantial and be seen as very similar to creating additional automated tests. Executing the static analyzers after students have solved an exercise would seem non-optimal as the students would probably not be engaged with the problem anymore and thus not interested in internalizing the feedback very deeply. To fully understand their applicability, additional research is required into how to implement them, how the students react to them and do they improve the students' learning or quality of the students' code.

Another approach for automatic code analysis could be the intelligent tutoring system (ITS), which generates more holistic feedback messages based on extensive student data. They are designed to simulate personalized tutoring assistance and provide help proactively during the students' solving process. Glassman et al. demonstrate MistakeBrowser and FixPropagator to analyze failed student solutions and to find the minimal number of transformations to fix them. With the systems, teachers can provide tailored feedback to all the students on the same problem\cite{glassman-reusable-feedback}. An ITS could provide advice whenever a student was unable to progress, writing unnecessarily complex or inefficient code, or in general when they wanted to receive advice. This could help in preventing students from developing bad programming habits. The implementation of an ITS would, however, require substantial effort and expertise \cite{glass-feature-engineering, its-2020}. 

The similarity detection methods produce either similarity or distance matrices, which we would want to cluster into meaningful groups for the teachers to use them for providing force-multiplied feedback. For this purpose, many of the staple clustering algorithms seem sufficient. Depending on the data, however, some of the algorithms might be less optimal to use since they have different preconditions how they expect the data to be distributed.

K-means algorithm would be suitable if the data was globular and easily partionable or if the teacher wanted to use a predefined number of clusters. DBSCAN would be a better choice for more asymmetric data that had clear dense regions surrounded by lower-density boundary regions. Yet as DBSCAN might not cluster well data of varying density, OPTICS or HDBSCAN might suit the task better. It is difficult to evaluate the optimal algorithm when the data might vary considerably between exercises or programming languages.

However, as demonstrated by Fox et al.~\cite{fox-clust-leverage-2015} and through experimentations with CodeClusters, a density-based clustering algorithm might be the most versatile approach for very heterogeneous data. Algorithms such as OPTICS or HDBSCAN are robust to varying density and they do not require a high level of expertise to set their parameters. But it should be noted that poor quality data would be impossible to cluster well regardless of the chosen clustering algorithm.

\bigskip
\noindent
\textbf{RQ3: How code submissions can be reviewed and explored efficiently?}
\bigskip

\noindent
If the baseline of reviewing and exploring submissions is going through them one by one to grade, provide feedback or to analyze them for patterns, there are various problems with this approach. With a high number of submissions this process is very time-consuming, susceptible to biases between different teachers, repetitive and inflexible in case the evaluation criteria is altered during the process. A grading rubric might help to maintain consistent evaluation criteria but it does not help to reduce the burden of the task. A solution could be to completely automatize the task by creating a static analyzer, which would discern the code with pattern matching to find various structures that could be used as indicators of possible problems.

However, creating these patterns with proper feedback is lacking in sophistication and it can in turn be as or more tedious as going through the submissions one by one. Also, many analyzers are not suited for pedagogical purposes and they work on individual programs, not on collections of similar code. Rather than attempting to manually define all the possible criteria how to review the submissions, a more flexible approach could be to reduce the dimensionality of the original data and letting teachers to be in control of the reviewing of the clusters. Reducing the code into numerical values, metrics, has been a subject of research since the 1970s but their use as for example indicators of complexity is hard to justify\cite{halstead-1972, mccabe-1976, cc-is-loc, shepperd-cc-1988}. Instead, using a similarity detection method the code could be discerned for similar structures which the teachers could evaluate and use for providing feedback. By receiving an overview of the higher-level structural approaches, the teachers might also gain a more in-depth understanding of the submissions while making the task quicker and more efficient than manual review.

For the detection of similarities various representations of program code could be used. For example ASTs contain the full hierarchical structure of the programs, CFGs the execution flow and token arrays the individual syntactical tokens, which can be enhanced by encoding the structure into tokens or using n-grams. Their similarities can then be scored with for example TED, GST or cosine similarity using vector space model. The optimal method would have to be quick even with thousands of submissions and also be able to detect varying levels of similarity. It has been suggested that the levels of similarity could be based on structural, syntactical and presentational levels\cite{luxton-sub-variation-2013}, yet additional research is required to verify how these could be utilized in the implementation of similarity detection models.

Various related systems or approaches have also been proposed to solve this very same problem. OverCode uses stacks\cite{overcode}, Codewebs semantic equivalence\cite{codewebs}, Divide and Correct custom distance metric\cite{divide-and-correct} and Fox et al. weighted TED\cite{fox-clust-leverage-2015}. While all of their approaches are very different, they share a common goal of reducing the dimensionality of the original data by clustering the submissions based on their similarities. All of the systems could be used as a basis for a new system and their approach improved, yet their undocumented or non-existent open-source code makes them hard to utilize.

Therefore, we contributed a new system, CodeClusters, which demonstrates a novel approach to review the submissions to understand how students have solved the exercises and to provide feedback. The features of the system include a search engine in the form of Apache Solr, which can be used for ad hoc search queries that should scale even to the largest of datasets, and a similarity detection model that generates n-grams of ASTs that are transformed into a TF-IDF matrix. Using the n-grams model teachers can cluster the code into structurally distinct cluster to analyze them or to provide feedback. CodeClusters also implements review flows, a data structure to automatize the search and modeling steps with default feedback message, as well as submission-to-review grid to provide an overview to detect which submissions have received what feedback. Further research is, however, required to confirm that this approach does indeed make the task of reviewing submissions more efficient than manual reviewing. Based on the experiences using CodeClusters and on related research, the approach should have distinct advantages especially for helping teachers to gauge general student programming patterns and solution schemas in a short amount of time.

\bigskip
\noindent
\textbf{RQ4: What are the main benefits and drawbacks of CodeClusters?}
\bigskip

\noindent
From the software engineering perspective, CodeClusters meets modern software engineering standards which makes it a great groundwork for additional improvements. Its open-source codebase is clear and well-documented, the deployment process is reproducible, and it uses various popular technologies such as React, Node.js, Postgres and Docker which should make new developers feel comfortable to commence further development.

From the teacher's point of view, CodeClusters offers exciting opportunities to improve their knowledge of the student programming patterns, to provide more personal feedback to students and to improve the test suites as well as course material. Its search functionality is based on Apache Solr, a popular and robust search server which allows the use of filters, facets and Lucene search syntax. Teachers can, for example, filter the submissions by a specific range of metric values to find outliers in which the students have possibly misunderstood a programming concept. The n-grams model CodeClusters implements executes quickly and it supports various parameters.

However, during the experimentation it became obvious that the implemented n-grams model is more suited for finding very similar submissions rather than higher-level approaches. With the test dataset, it clusters the submissions based on minor differences of syntax such as the use of integers instead of doubles or \texttt{if} instead of \texttt{else if}. Also, the model can be complicated to use as it allows the tuning of various parameters that require some expertise to set appropriately. Review flows are a unique CodeClusters feature to allow composition of search, modeling and feedback messages as single executable actions, yet which require additional software development to complete their functionality. The submission-to-review grid provides an easy way to visualize the submissions and the reviews they have received.

CodeClusters was tested with a considerably small test dataset, 110 Java submissions, on which it performed efficiently. However, it is possible that bottlenecks exist that would make operations such as running the n-grams model slow although this was not investigated by using a larger dataset. The scaling of the system should otherwise be relatively easy, as increasing the number of the server instances should be enough to increase the system's capacity to process requests. Showing large numbers of submissions in the UI was noticed to cause latencies with the rendering of React components.

Although the system is more of a prototype than complete system, the first impressions of teachers were positive regarding CodeClusters but to verify its applicability in full, further development and research is needed. One design goal for CodeClusters is flexibility so that instead of a one fixed approach, teachers could compose different approaches such as search queries and models. This should provide the teachers freedom to experiment, analyze and automate different workflows to efficiently review and explore the submissions. 

Related systems and research have shown that the analyzing and providing force-multiplied feedback with a clustering approach can work to help teachers process vast amounts of program code submissions. Providing feedback during the solving of the exercise would be preferred compared to one that is given later, and perhaps an approach different from CodeClusters' would be needed to provide feedback. Nevertheless, CodeClusters allows teachers to analyze and search the submissions with a straightforward user interface which, by far and large, is not possible with the current systems.

\section{Limitations of the study}

The major problem of the study is its low sample size that cannot be considered as a comprehensive representative of the whole population of programming course teachers. Also, since the interviews did not go into too much detail of the applicability of the CodeClusters nor teachers could use it with the programming courses they teach, this limits the extent what can be interpreted from the results. While every teacher thought the artifact and the idea to be interesting and worthwhile to investigate, the study did not produce conclusive data on how the search or modeling functionality would help teachers. For confirming the problem and guiding the next design direction of the artifact the study is sufficient, yet more development and research is required to verify the artifact and its approach.

\section{Future work}

We have found many possible areas of future work regarding CodeClusters and research. Adding integrations to LMSs would be necessary to trial it on programming courses and the UI could be improved. Allowing side-by-side comparisons between clusters would be important but also a visualization to show the submissions in a more compact form. As of now, even with moderately sized clusters teachers would still have to go through a considerable number of submissions individually to discern the underlying structural patterns of a cluster. Perhaps showing a structural skeleton of the submissions, a CFG perhaps, could be a one approach, or collapsing the code by similar segments and showing only where they differ, akin to a \texttt{diff} tool.

It might be a good idea to research further how to develop a similarity detection model that is especially tailored for detecting program code similarities of varying levels. Vector space model that uses various features instead of just AST tokens and n-grams might work, yet conducting manual review first might be necessary to understand how the teachers would even prefer the submissions to be clustered. The improvements to the n-grams model could include generating the n-grams hierarchically instead of sequentially, tuning the set of used tokens and using both AST and CFG graphs. After developing the model to a high enough accuracy, it could be simplified in the UI so that the teachers could achieve good results without having to tune the parameters of the model. This would allow them to focus on the analysis of the clusters rather than on the data science of the models.

Other interesting areas include designing a system that provides more integrated feedback than one that is sent after students have solved the exercise. Students would probably find it more motivating to have the system work during their solving process rather than after it, which would require a different approach for providing feedback as proposed by CodeClusters. The benefits of adding a static analyzer might be good to study to understand if the students' comprehension and ability to write good quality code increases. Developing an ITS might be the ultimate approach for providing students with holistic and versatile feedback, yet it requires considerable effort to implement properly.
