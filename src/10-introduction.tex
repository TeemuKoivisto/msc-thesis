Thousands of programming courses use programming exercises as part of their curriculum. It is widely accepted that hands-on exercises help students to understand and learn concepts more deeply than without them. A crucial part of exercises is grading them, so that the students can confirm they have correctly understood a given concept. Another important part is providing feedback to students, to guide them to improve their work to better match the requirements for a given exercise \cite{survey-feedback-gen, doing-feedback-well, towards-providing-feedback}.

On these assumptions, it is then highly desirable to grade the exercises students have submitted and to provide feedback to students. The most primitive approach would be to manually grade all of the student submissions and to provide individual feedback to every student. This, however, faces the limits of the teacher's capability to review code. It is not uncommon for a course to receive hundreds or even thousands of submissions each week \cite{overcode, codewebs, stanford-million, fox-clust-leverage-2015}. Even with the help of multiple teaching assistants, the scale of the problem is considerable and the reviewing often monotonous.

A common solution to this problem is to use automated tests \cite{tmc, automatic-tests, aalto-2010-auto-ass-systems-review, aalto-2001-auto-asses, alamutka-2005-auto-ass-survey}. For every exercise, teacher would create a set of tests that would validate the submission on a series of test inputs and output the results of each test. These results would then be sent to the students who could then use it as feedback to improve their code. The passed and failed tests could be quantified into a single numeric value as the test score. With this type of system, it eliminates the need for teachers to manually review and grade the exercises. Automatic grading could also be more consistent due to the objectivity of tests, faster and have around the clock availability \cite{ala-mutka-2003-caa-review, alamutka-2005-auto-ass-survey}. After creating the proper test-suites and pipeline, the time required to manage the system would become minimal \cite{auto-ass-tools-2017, alamutka-2005-auto-ass-survey}.

\iffalse
alamutka2005
Teachers often agree that it is not possible to assess automatically all the issues relating to good programming.

With larger assignments, a more common approach seems to be to combine manual and automatic assessment.
\fi

This solution, however, involves a trade-off. While the automated tests would remove the need for teachers to manually assess the code, they could stop reviewing the submissions all together. They would then have to trust the tests to capture all the possible variations of incorrectness and this also might lead to teachers becoming unsure about the programming patterns of their students and force them to rely on the test scores to indicate any gaps in the students' understanding.

Of course, in most cases, teachers would have other ways to analyze their students' behavior. If student assistance chat channels, forums or lab sessions exist, teachers could use them to gauge student misconceptions and problems \cite{multi-faceted-mooc-support, glassman-reusable-feedback}. Having the historical data on attempted submits showing each failed test would be helpful in knowing what problems the students have struggled the most with \cite{glassman-reusable-feedback}. In some cases, there are even tools in use that show snapshots of students' code to display their progress towards solving an exercise \cite{snapshots}. Still, none of these approaches provide a synthesis of the higher-level patterns of the student code.

It has been suggested that when the exercises are automatically assessed some students might start to optimize their effort to do only the bare minimum to pass them, disregarding quality aspects \cite{aalto-2010-auto-ass-systems-review}. It might also tempt the students to attempt tricking the automatic grading system \cite{alamutka-2005-auto-ass-survey}. Hence, if the students knew that their code would be reviewed by the teacher, it might incentivize them to better uphold the quality of their code as well as to discourage the students from bypassing the tests. Perhaps even plagiarism would decrease with greater teacher oversight.

Other ways to grade the exercises exist that would eliminate the need for the teachers to review code, such as self-grading or peer grading, yet they have not been proven as reliable as teacher-authored grading \cite{self-peer-grading, codewebs, divide-and-correct}. If we expected teachers to be able to access the code and review it, they would still have to face the same problem as with the manual reviewing - there are too many submissions to go through one by one \cite{overcode, codewebs}.

The problem of analyzing a large set of documents to find interesting patterns and to provide feedback, could be labelled as an information retrieval problem. Information retrieval (IR) is a vast topic entailing the fundamentals of finding the most relevant information out of a large dataset, as well as how it could be built into a software system \cite{ir-in-practise, intro-to-ir}. Google Search\footnote{\url{https://www.google.com/}} is perhaps the most ubiquitous IR system, which proves that the search engine is a versatile and robust tool for searching huge datasets. Search engines have also been applied to the searching of code from git repositories\footnote{\url{https://github.com/search}} but would it be possible to apply it for student code submissions? More importantly, would it even solve the problems teachers face?

If we thought the search engine as a simple full-text search, the teachers could find various data structures, libraries or patterns with pattern-based matching. They could be then used as indicators for possible misconceptions or distinct structural patterns. However, to resolve all their reviewing with text searches would be impractical, as inventing new search queries would be time-consuming and their use for discerning higher-level structural patterns unreliable. Instead, a much faster approach could be to find the similar patterns automatically and letting teachers review them as groups. This could immediately visualize the variations between the submissions without burdening the teachers with extensive manual review. The similarity could be based on the solution schemas students have used to solve the exercises which are defined by their distinct use of tokens and control-flow, or using other, smaller patterns\cite{overcode}. Additionally, teachers could be interested in finding the submissions that are very far apart from the rest - the outliers.

Systems have been proposed over the years to solve this problem by reducing the dimensionality of the data to allow teachers to manage the submissions as groups instead of singular submissions. A custom search engine, Codewebs \cite{codewebs}, was developed by Nguyen et al. to enable teachers to quickly search and explore the submissions and to provide feedback based on the Abstract Syntax Trees (ASTs) of the submitted code. OverCode by Glassman et al.~\cite{overcode} is a student code reviewing and clustering system that uses unique "stack" data structures to divide the submissions into clusters. For natural language submissions, Divide and Correct by Brooks et al.~\cite{divide-and-correct} is a system to grade and provide feedback to student submissions in MOOCs using a predefined number of clusters and subclusters to keep them within manageable limits. These systems are discussed in more detail in the Section~\ref{sec:related-systems}.

Curiously, none of the systems have seen much development since their creation. Why that is can be speculated about, but this would indicate that there exists a need for this type of system. Being able to review and provide force-multiplied feedback to a large number of student submissions quickly, were there automated tests or not, could be very useful if it would not overburden the teachers with too much work \cite{aalto-2010-auto-ass-systems-review}. This is also relevant to natural language submissions, and various methods to automatize their grading and providing feedback have been researched \cite{auto-essay-scoring-case-studies, arxiv-auto-essay-grading-feedback-2018 ,divide-and-correct}. Outside the educational context, commercial services have been built that offer automated code review as a service that are somewhat similar\footnote{\url{https://aws.amazon.com/codeguru/}}. The clustering of program code in general is a quite large topic, but most of its applications have been focused on the detection of plagiarism and clones and not for educational purposes.

This thesis examines the basics of constructing a system to review and cluster student code with an ability to provide force-multiplied feedback to the students. The main question it aims to answer is: "How a system could be built to efficiently explore, analyze and provide feedback to student code submissions in programming courses?" Because this topic is very large, it is divided into four research questions:

\addtolength\leftmargini{6pt}
\begin{itemize}
    \item[\textbf{RQ1:}] How code can be represented for efficient search and modeling?
    \item[\textbf{RQ2:}] What methods can be used to analyze and cluster code?
    \item[\textbf{RQ3:}] How code submissions can be reviewed and explored efficiently?
    \item[\textbf{RQ4:}] What are the main benefits and drawbacks using CodeClusters for reviewing and exploring code?
\end{itemize}

The methods to answer them include a literature review of the fields of clustering, analysis, information retrieval, similarity detection as well as representations of code. They were chosen based on the connections drawn from the related systems as well as from having researched these topics in-depth. As an additional contribution this thesis presents a new system, CodeClusters, that implements some of these ideas and methods in practice. Exploratory design science research (DSR) analysis is applied to assess its usability and user interface (UI), and the results are used to validate its design as well as to guide the next iteration of the artifact. Also, we evaluate the problem context of teachers not being able to review the large quantities of submissions in programming courses, and the applicability of reviewing code using full-text search and n-grams based model.

As the study method, we conduct a series of interviews with teachers who have taught programming courses. Based on the interview recordings, transcripts and questionnaire answers, a thematic synthesis is carried out of the results of the interviews to find the main topics. Additionally, UI and system design criteria are used to evaluate the artifact. Then, they are used in conjunction with the literature review, the experiences developing the system and the reviewing of related systems, to answer the research questions. The goals of this thesis are to validate the problem context, the applicability of search, similarity detection and clustering for discerning patterns from submissions as well as the usability of CodeClusters.

This thesis was written as part of Data Science Master's programme at the University of Helsinki, but the software was developed for Aalto University's LeTech research group and open-sourced under the permissive MIT license \footnote{\url{https://github.com/Aalto-LeTech/CodeClusters}} \footnote{\url{https://github.com/Aalto-LeTech/CodeClustersModeling}}.

The next chapter conducts a literature review into the research fields that intersect with our topic. In Chapter~\ref{ch:system-description}, the proposed system CodeClusters is described and also a few of the related systems are reviewed. In Chapter~\ref{ch:methodology}, the research design and the structure of the interviews are described, with Chapter \ref{ch:results} and Chapter \ref{ch:discussion} containing the results and their discussion. In Chapter~\ref{ch:conclusions}, conclusions are drawn of the overall results of this thesis and its implications.
