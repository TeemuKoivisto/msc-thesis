Similarity is an abstract concept that is hard to describe in exact terms. Philosophically, similarity of objects is only a matter of framing - all submissions are code and therefore similar. Yet for us a more precise definition would be more usable.

We shall define similarity as the similarity of the documents' elements and structure when they are represented as trees, graphs or arrays. We normalize this similarity value $s$ as $\{ s \in \mathbb{R} \mid 0\leq s \leq 1\}$ where 1 indicates equality and 0 total dissimilarity. The range $[0,1]$ is arbitrary but chosen for its good mathematical properties. In this context similarity distance would mean the opposite of similarity, which can be defined as $d = 1 - s$. Similarity detection would be the applying of a similarity scoring function, such as $sim : (a, b) \rightarrow [0,1]$, for a set of documents $D=\{d_1, d_2, ..., d_n\}$ to create a similarity matrix with the scores computed for every document in relation to every other document, as shown in the IR Section Equation \ref{eq:sim-matrix}.

Most of the research in similarity detection of program code has focused on the detection of plagiarism, software license violations and clones. To differentiate between the similarity types, three levels have been proposed: purpose, algorithm and implementation level \cite{zhang-towards-plag-det}.

If two programs both sort numbers by ascending order, they can be considered similar at the purpose level. If they share the same algorithm, at algorithm level, and if their implementation of the algorithm is the same, at implementation level. Due to the difficulty of deciding whether two programs are equal at the purpose or algorithm level, most similarity methods work only at the implementation level \cite{chaiyong-2018}. If we expect every student to have solved the same exercise, we could already assume them to be same at the purpose level.

Similarity detection has been a popular approach for providing clustered feedback to students \cite{overcode, codewebs, divide-and-correct, fox-clust-leverage-2015, fox-roy-autostyle-msc-2016, stanford-million, survey-feedback-gen}. An explanation could be that it closely simulates the natural process how humans would group the submissions together if they were to provide shared feedback to many submissions at once. An alternative could be to use classifiers to provide feedback or rule-based analyzers. Deciding how the similarities are calculated is crucial in producing accurate results, yet as discussed in the Section~\ref{sec:analyzing-code}, there might be multiple similarity levels and their measurement would be susceptible to subjective biases by human reviewers.

Systems such as OverCode, Codewebs and Divide and Correct all use similarity detection and their results are used to cluster the submissions. Many of the systems use derivations of ASTs as their representation, but the used similarity measures vary a lot. OverCode uses a custom algorithm that matches the canonicalized stacks together, Codewebs uses probabilistic semantical equivalence to find the most used context for given AST subtree and Divide and Correct uses multiple feature vectors which are compared using cosine similarity \cite{overcode,codewebs,divide-and-correct}.

The trade-offs of each method are their accuracy and generalizability outside that specific dataset, yet it is hard to say what method would be the most balanced given the wide range of possible data and representations. The most flexible approach could be to offer multiple methods that can detect similarity at different levels, which we discussed previously in the Section \ref{sec:analyzing-code}.

Another use case for similarity detection is plagiarism detection, which is not discussed in the context of this thesis. With good similarity detection mechanism finding very similar student submissions should be trivial but applying it to publicly available code on internet would be challenging \cite{simon-better-ngrams-2020}.

Next, we present a few of the most popular and interesting similarity detection methods for code, namely tree edit-distance, greedy string tiling and n-grams. A major area that is omitted is neural network based systems, such as Oreo~\cite{oreo}, since while promising they require very in-depth technical analysis which is not possible in the scope of this thesis.

\subsection{Tree edit-distance}
\label{ssec:ted}

Its best-known version proposed by Zhang et al. (1989) \cite{zhang-et-al-1989} tree edit-distance (TED) is the minimum number of edit operations required to transform a tree into another tree. The operations to edit the tree in the original paper were threefold: \texttt{insert}, \texttt{delete} and \texttt{rename}. Some versions might use more operations.

Since TED only computes the distance between two trees, for similarity detection the algorithm has to be computed for all the unique pairs of documents. These are for a set of documents $D$ all the combinations of two $\binom{D}{2}$. This results in a symmetric edit-distance matrix with diagonal values of 0, the edit-distances being positive integers $e \in \mathbb Z_{\ge 0}$. They could be normalized into similarity distances between $[0, 1]$ by dividing them by the largest edit-distance, but since the relationship would stay linear this only matters if the clustering algorithm requires it.

The algorithm itself is quite long and complicated and it would need substantial explanations on its behavior, so we shall omit a detailed analysis and summarize it as a recursive function with two inner loops that store the calculated distances in a large edit-distance matrix, and backtracks the shortest path once the matrix has been completed. But unless optimizations are in place the algorithm will iterate over every possible combination the tree $a$ could be transformed into tree $b$ \cite{zhang-et-al-1989, ted-tutorial-2018}.

Zhang et al. note that the worst-case time complexity of the algorithm is $O(m^2\cdot n^2)$ where $m$ and $n$ are the number of nodes in the trees, which with two equally sized trees would become $O(n^4)$. Using optimizations, such as hash tables and dynamic programming, the space complexity of the algorithm becomes however $O(mn)$ \cite{zhang-et-al-1989, ted-tutorial-2018}. Later advancements have brought the time complexity to $O(n^2m(1+\log \frac{m}{n}))$ \cite{ted-demaine}.

This makes applying it quite problematic as it does not scale very well to larger datasets. Many plagiarism detection and educational clustering systems use TED as their similarity detection algorithm, yet many note its poor scalability to be one of the main issues with the method
\cite{stanford-million, fox-clust-leverage-2015, fox-roy-autostyle-msc-2016}. 

Its benefits lie in its hierarchical behavior; for example, removing a higher-level node with many children will take multiple operations, as every child has to be removed or moved first. This automatically weights the algorithm so that structures higher in the tree have larger impact on the similarity calculation. Also, while the worst-case complexity in theory is quartic, Nguyen et al. note that in practise for their student submissions it is only quadratic \cite{stanford-million}.

To improve the detection of structural differences, it might still be better to add a separate cost function that would define different costs for different operations and nodes. For example, removing lower-level node such as variable assignment should probably have a lower weight than removing a structure such as \texttt{class} or \texttt{function}. Fox et al. propose this weighted TED for their similarity detection of programming submissions and note that it performs better than the non-weighted version \cite{fox-clust-leverage-2015}.

For CodeClusters we tested TED with our moderately sized 110 submission test dataset, yet the algorithm performed very slowly taking approximately 10 minutes each execution. With better, lower-level and parallelized implementation it could perhaps be a lot faster to run but due to the limited available resources to fix it, it was not included.

\subsection{Greedy string tiling}
\label{ssec:gst}

Another popular method for plagiarism detection is greedy string tiling (GST) algorithm, originally proposed by Wise (1993) \cite{wise-gst-1993}. As indicated by its name it is commonly used with strings, but since a string is only an array of characters it can be modified to work with token arrays. Two of its most popular implementations are the JPlag and YAP3 plagiarism detection libraries \cite{jplag, yap3}

The issues with GST are similar to TED which are its high worst-case complexity and inability to differentiate with token types or their operations. GST has a worst-case complexity of $O(n^3)$ but using an optimization in the form of Karp-Rabin algorithm to store the subparts of input strings, or arrays, as hash values in a hashtable, the practical worse case becomes between $O(n)$ and $O(n^2)$ \cite{wise-gst-1993}. Yet unlike TED, GST does not allow weighting of the tokens which makes it less flexible for structural similarity detection.

For CodeClusters, we evaluated GST but the time it would have required to properly implement it was deemed too long. Also, the problems with TED indicated that the high worst-case complexity might be a big problem. By analyzing the clusters from the TED and n-grams models, we found that the algorithms detected mainly presentational similarities and not very deep structural patterns. This could mean GST is especially poor fit due its limited capability in detecting higher-level structure.

\subsection{N-grams}
\label{ssec:ngrams}

N-grams is not exactly a similarity detection method in itself, but a method to represent strings or arrays in a form that captures some of their sequential structure. A formal definition would be that n-grams of a string are all the $n$ length contiguous substrings. For example, n-grams of string "aabc" would be $\texttt{2-grams}(\text{"aabc"})=\{\text{"aa"}, \text{"ab"}, \text{"bc"}\}$ with $n=2$ \cite{ir-in-practise, chaiyong-2018}.

N-grams is widely popular in natural language plagiarism detection and as a method to represent text in general, and they are often used with vector space representation which was described in the Subsection \ref{ssec:vsm}. The similarity between two vectors could be measured with various methods, but the most popular similarity measure is cosine similarity \cite{ir-in-practise, chaiyong-2018}.

The main strengths of n-grams are its flexibility and efficiency. Compared to TED and GST, generating n-grams for a document is a $O(n)$ operation that is computed only once per document, not every document pair. After n-grams have been generated they can be transformed into a TF-IDF matrix which can be processed with vectorized operations.

The weaknesses of array representation were mentioned in the Subsection \ref{ssec:array}, which come down to its lack of hierarchical structure that makes discerning higher-level patterns difficult. What n-grams is capable of, however, is discerning local sequential structure of the tokens which can be combined with the results of multiple different sized n-grams. Using specific tokens to indicate start and end of statements work to some extent as demonstrated by JPlag, yet they are lacking in power.

A novel improvement for AST-based n-grams is proposed by Karnalim and Simon which instead of creating n-grams sequentially from the array, it generates them following the hierarchical structure of the tree. For example, a sequence of tokens such as \texttt{PARENT}, \texttt{CHILD\_1}, \texttt{GRANDCHILD}, \texttt{CHILD\_2} would under sequential processing of 2-grams become: \texttt{(PARENT, CHILD\_1)}, \texttt{(CHILD\_1, GRANDCHILD)} and \texttt{(GRANDCHILD, CHILD\_2)}. Whereas hierarchical processing would generate: \texttt{(PARENT, CHILD\_1)}, \texttt{(PARENT, CHILD\_2)} and \texttt{(CHILD\_1, GRANDCHILD)} \cite{simon-better-ngrams-2020}.

CodeClusters uses n-grams as its main similarity detection method which works moderately well, although the clusters it produces are perhaps even too similar leaving out a large portion of the dataset unclustered. Therefore, the granularity of the used ASTs should probably be lowered to better capture structural aspects instead of presentational variations. Using hierarchical n-grams might also result in improvements.
